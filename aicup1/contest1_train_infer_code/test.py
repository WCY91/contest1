# -*- coding: utf-8 -*-
import torch
import numpy as np
from monai.networks.nets import SwinUNETR
from monai.losses import DiceFocalLoss
from torch.cuda.amp import autocast, GradScaler

def main():
    # ---------------- åŸºæœ¬è¨­å®š ----------------
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"âœ… ä½¿ç”¨è£ç½®: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}")

    # æ¨¡æ“¬è·Ÿè¨“ç·´ä¸€è‡´çš„è¨­å®š
    roi_size = (128, 128, 96)
    num_classes = 4
    feature_size = 48

    # ---------------- æ¨¡å‹ ----------------
    model = SwinUNETR(
        img_size=roi_size,
        in_channels=1,
        out_channels=num_classes,
        feature_size=feature_size,
        use_checkpoint=False
    ).to(device)
    model.train()

    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=2e-4)
    scaler = GradScaler(enabled=True)

    # ---------------- å»ºç«‹éš¨æ©Ÿè¼¸å…¥è³‡æ–™ ----------------
    B = 1
    img = torch.randn(B, 1, *roi_size, device=device)
    lab = torch.randint(0, num_classes, (B, 1, *roi_size), device=device).long()

    # ---------------- å»ºç«‹ DiceFocalLoss ----------------
    alpha_tensor = torch.tensor([0.05, 0.25, 0.35, 0.35], device=device)
    lam_focal = 0.3

    loss_fn = DiceFocalLoss(
        to_onehot_y=True,  # label è‡ªå‹• one-hot
        softmax=True,      # output èµ° softmax
        lambda_dice=0.7,   # Dice æ¬Šé‡
        lambda_focal=lam_focal,  # Focal æ¬Šé‡
        gamma=1.5,         # Focal gamma
    )

    # ---------------- Forward æ¸¬è©¦ ----------------
    print("ğŸš€ é–‹å§‹æ¸¬è©¦ DiceFocalLoss forward/backward ...")
    with autocast(enabled=True):
        out = model(img)
        loss = loss_fn(out, lab)

    print(f"âœ… Loss æ­£å¸¸è¨ˆç®—: {loss.item():.6f}")
    print(f"   è¼¸å…¥å½±åƒå½¢ç‹€: {tuple(img.shape)}")
    print(f"   æ¨¡å‹è¼¸å‡ºå½¢ç‹€: {tuple(out.shape)}")
    print(f"   æ¨™ç±¤ shape: {tuple(lab.shape)} | dtype={lab.dtype}")

    # ---------------- Backward æ¸¬è©¦ ----------------
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
    print("âœ… Backward / Optimizer step å®Œæˆï¼")

    print("ğŸ‰ æ¸¬è©¦æˆåŠŸï¼šDiceFocalLoss åœ¨ MONAI 1.2.0 ä¸‹é‹ä½œæ­£å¸¸ï¼")

if __name__ == "__main__":
    import torch.multiprocessing as mp
    mp.freeze_support()
    main()
